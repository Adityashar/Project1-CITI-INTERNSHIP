{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_data():\n",
    "    # get the name of the new dataset file\n",
    "    \n",
    "    directory = '/home/aditya/Documents/citibot/newdata'\n",
    "    data_name = \"table.csv\"\n",
    "    DATASET_LOC = os.path.join(directory, data_name)\n",
    "\n",
    "    # read the data file - csv, excel and json\n",
    "\n",
    "    if data_name.endswith('.csv'):\n",
    "        DATASET = pd.read_csv(DATASET_LOC)\n",
    "    elif data_name.endswith('.xlx') or data_name.endswith('.xlsx'):\n",
    "        DATASET = pd.read_excel(DATASET_LOC)\n",
    "    elif data_name.endswith('.json'):\n",
    "        DATASET = pd.DataFrame(json.load(open(DATASET_LOC, 'r')), index =[1])\n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should be triggered by a listener\n",
    "\n",
    "DATASET = new_data()\n",
    "\n",
    "# dict of columns in new data with corresponding dtypes\n",
    "\n",
    "DATASET['Specimen No'] = DATASET['Specimen No'].astype('str')\n",
    "DATASET.columns =[\"column \" + str(i) for i, column in enumerate(DATASET.columns)] \n",
    "DATASET.columns =[column.replace(\" \", \"_\") for column in DATASET.columns] \n",
    "\n",
    "FEATURES = {col: DATASET[col].dtype for col in DATASET}\n",
    "DATASET.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the possible entities \n",
    "def get_entities(threshold_value):\n",
    "    PRIMARY_KEY = []\n",
    "    ENTITIES = {}\n",
    "\n",
    "    for col in FEATURES.keys():\n",
    "        if (DATASET[col].unique().shape[0] == DATASET.shape[0]) and (FEATURES[col] == 'O' or FEATURES[col] == 'int64'):           \n",
    "            PRIMARY_KEY.append(col)                                             # Make this the PRIMARY KEY\n",
    "            ENTITIES[col] = DATASET[col].unique()[:threshold_value].tolist()\n",
    "\n",
    "        elif DATASET[col].unique().shape[0] > threshold_value and (FEATURES[col] == 'O' or FEATURES[col] == 'int64'):\n",
    "            ENTITIES[col] = DATASET[col].unique()[:threshold_value].tolist()\n",
    "\n",
    "        elif FEATURES[col] == 'O' or FEATURES[col] == 'int64':\n",
    "            ENTITIES[col] = DATASET[col].unique().tolist()\n",
    "            \n",
    "    return ENTITIES, PRIMARY_KEY\n",
    "\n",
    "threshold_value = 20\n",
    "ENTITIES, PRIMARY_KEY = get_entities(threshold_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For query generation, any number of previously determined entities (values) can be used.\n",
    "# For each intent, what should differ between the different sets of queries is the meaning the latter conveys.\n",
    "# For eg if we have an intent \"Cost\", it should have questions like :\n",
    "# what is the cost {intent} for entity_1{value} ?\n",
    "# Since we have to use the intent / Feature name as a part of the query itself, an important thing to conside is \n",
    "# that the feature names should be simple and directly convey their purpose\n",
    "# For eg, feature name can be \"cost\" but if the feature name is \"cost per head\" then we the issue is that\n",
    "# the intent name wont be the same as the feature name, once this happens we'll have to extract the real intent \n",
    "# from the feature names which is not possible to do right now.\n",
    "\n",
    "def get_questions(intent):\n",
    "    questions = []\n",
    "    questions.append(\"what is {} for {}\".format(intent))\n",
    "    questions.append(\"Tell me something about {} with {}\".format(intent))\n",
    "    questions.append(\"Give me information about {} for the {}\".format(intent))\n",
    "    \n",
    "    return questions\n",
    "\n",
    "# Dict of intents having a list of questions as their values.\n",
    "\n",
    "INTENTS = {col:get_questions(col) for col in FEATURES.keys()}\n",
    "INTENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 0]\n"
     ]
    }
   ],
   "source": [
    "from actions import lis\n",
    "\n",
    "print(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the current nlu.md file has the following information : (1)intents, (2)lookup, (3)synonyms\n",
    "# created three functions to update each of the these sections respectively\n",
    "# optimisation for future instead of writing, just append new data\n",
    "\n",
    "\n",
    "def synonyms_to_md(data_md, entity_dict):\n",
    "    data = []\n",
    "    for entity in entity_dict.keys():\n",
    "        if FEATURES[entity] == 'int64' or entity in PRIMARY_KEY:\n",
    "            continue\n",
    "            \n",
    "        # Now the only entities left are character dtype with limited uniques\n",
    "        # For each value in entity add synonyms\n",
    "        \n",
    "        for val in entity_dict[entity]:\n",
    "            string = \" synonym:{}\\n\".format(val)\n",
    "            synonyms = [val.lower(), val.upper(), val.title()]\n",
    "            for sys in synonyms:\n",
    "                string += \"- {}\\n\".format(sys)\n",
    "            string += \"\\n\"\n",
    "            data.append(string)\n",
    "    \n",
    "    data_md += data\n",
    "    return data_md\n",
    "\n",
    "# lookup should not include primary_key entity, integer entities\n",
    "\n",
    "def lookups_to_md(data_md, entity_dict):\n",
    "    data = []\n",
    "    for entity in entity_dict.keys():\n",
    "        string_ent = \" lookup:{}\\n\".format(entity)\n",
    "        \n",
    "        if FEATURES[entity] == 'int64' or entity in PRIMARY_KEY:\n",
    "            continue\n",
    "            \n",
    "        for val in entity_dict[entity]:\n",
    "            string_ent += \"- {}\\n\".format(val)\n",
    "        string_ent += \"\\n\"\n",
    "        data.append(string_ent)\n",
    "    \n",
    "    data_md += data\n",
    "    return data_md\n",
    "\n",
    "def intents_to_md(data_md, intent_dict):\n",
    "    data_md[-1] = data_md[-1] + '\\n'\n",
    "    data = []\n",
    "    for intent in intent_dict.keys():\n",
    "        string_intent = \" intent:{}\\n\".format(intent)\n",
    "        for ques in intent_dict[intent]:\n",
    "            string_intent += \"- {}\\n\".format(ques)\n",
    "        string_intent += \"\\n\"\n",
    "        data.append(string_intent)\n",
    "    \n",
    "    data_md += data\n",
    "#     print(data)\n",
    "    return data_md\n",
    "\n",
    "nlu = open('/home/aditya/Documents/citibot/newdata/nlu.md', 'r')\n",
    "s=nlu.read().split('##')\n",
    "\n",
    "nlu_intent = intents_to_md(s, INTENTS)\n",
    "nlu_lookup = lookups_to_md(nlu_intent, ENTITIES)\n",
    "# nlu_synonyms = synonyms_to_md(nlu_lookup, ENTITIES)\n",
    "\n",
    "new_nlu = '##'.join(nlu_lookup)\n",
    "\n",
    "f = open(\"demofile2.md\", \"w\")\n",
    "f.write(new_nlu)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the current stories.md file has storylines with respect to a classified intent\n",
    "# For the dynamic addition of data, contextual storylines cannot be added due to their increased complexity\n",
    "# name : ## {intent} path\n",
    "\n",
    "def new_stories(story, intent_dict):\n",
    "    data = []\n",
    "    actions = []\n",
    "    for intent in intent_dict:\n",
    "        action = \"action_{}\".format(intent)\n",
    "        string =\" {} path 1\\n* {}\\n  - {}\\n\\n\".format(intent, intent, action)\n",
    "        data.append(string)\n",
    "        actions.append(action)\n",
    "        \n",
    "    story += data\n",
    "    return story, action\n",
    "    \n",
    "story = open('/home/aditya/Documents/citibot/newdata/stories.md', 'r')\n",
    "s=story.read().split('##')\n",
    "\n",
    "story_text, Actions = new_stories(s, INTENTS)\n",
    "new_story = '##'.join(story_text)\n",
    "\n",
    "f = open(\"story.md\", \"w\")\n",
    "f.write(new_story)\n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', ' intent:greet\\n- hey\\n- hello\\n- hi\\n- good morning\\n- good evening\\n- hey there\\n\\n', ' intent:hello_world\\n- hello world\\n- programming\\n- c++\\n- java\\n\\n', ' intent:goodbye\\n- bye\\n- goodbye\\n- see you around\\n- see you later\\n\\n', ' intent:affirm\\n- yes\\n- indeed\\n- of course\\n- that sounds good\\n- correct\\n\\n', \" intent:deny\\n- no\\n- never\\n- I don't think so\\n- don't like that\\n- no way\\n- not really\\n\\n\", \" intent:mood_great\\n- perfect\\n- very good\\n- great\\n- amazing\\n- wonderful\\n- I am feeling very good\\n- I am great\\n- I'm good\\n\\n\", ' intent:mood_unhappy\\n- sad\\n- very sad\\n- unhappy\\n- bad\\n- very bad\\n- awful\\n- terrible\\n- not very good\\n- extremely sad\\n- so sad\\n\\n', ' intent:bot_challenge\\n- are you a bot?\\n- are you a human?\\n- am I talking to a bot?\\n- am I talking to a human?\\n\\n', ' intent:mycolor\\n- I will choose the color [green](color)\\n- how is [blue](color)\\n- color is [black](color)\\n- color was [blue](color)\\n\\n', ' lookup:color\\n- green\\n- black\\n- blue\\n- red \\n- orange\\n\\n', ' lookup:location\\n- rajasthan\\n- uttar Pradesh\\n- haryana\\n- madhya Pradesh\\n- telangana\\n- punjab\\n- kerala\\n- karnataka\\n- maharashtra\\n\\n', ' intent:corona_state\\n- Give me details about [Karnataka](location)\\n- Give me details about [haryana](location)\\n- What is covid stats of [Kerala](location)\\n- covid stats of [Tamil Nadu](location)\\n- covid stats of [rajasthan](location)\\n- covid stats of [maharashtra](location)\\n\\n', ' intent:mydate\\n- what was the day on [2 january](date)\\n- what was the day on [22/10/20](date)\\n- what was the day on [29-11-1991](date)\\n- today is [10-3-2020](date)\\n\\n', ' intent:heyaaaa\\n- how\\n\\n', ' intent:how\\n- heyaaaa\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "mdf = open('/home/aditya/Documents/citibot/newdata/nlu.md', 'r')\n",
    "# markdown.markdown(mdf.read())\n",
    "s=mdf.read().split('##')\n",
    "\n",
    "h, p = \"heyaaaa\", \"how\"\n",
    "string = \"\"\n",
    "\n",
    "string1 = \" intent:{}\\n\".format(h)\n",
    "string1 += \"- {}\\n\".format(p)\n",
    "string1 += \"\\n\"\n",
    "\n",
    "string2 = \" intent:{}\\n\".format(p)\n",
    "string2 += \"- {}\\n\".format(h)\n",
    "string2 += \"\\n\"\n",
    "ss= []\n",
    "ss.append(string1)\n",
    "ss.append(string2)\n",
    "print(ss)\n",
    "v = '##'.join([string, string1, string2])\n",
    "\n",
    "s[-1] = s[-1] + '\\n'\n",
    "s += ss\n",
    "print(s)\n",
    "v = '##'.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
