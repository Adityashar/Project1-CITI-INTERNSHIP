{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import markdown\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_data(data_name, directory):\n",
    "    # get the name of the new dataset file\n",
    "    \n",
    "    DATASET_LOC = os.path.join(directory, data_name)\n",
    "\n",
    "    # read the data file - csv, excel and json\n",
    "\n",
    "    if data_name.endswith('.csv'):\n",
    "        DATASET = pd.read_csv(DATASET_LOC)\n",
    "    elif data_name.endswith('.xlx') or data_name.endswith('.xlsx'):\n",
    "        DATASET = pd.read_excel(DATASET_LOC)\n",
    "    elif data_name.endswith('.json'):\n",
    "        DATASET = pd.DataFrame(json.load(open(DATASET_LOC, 'r')), index =[1])\n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_0</th>\n",
       "      <th>column_1</th>\n",
       "      <th>column_2</th>\n",
       "      <th>column_3</th>\n",
       "      <th>column_4</th>\n",
       "      <th>column_5</th>\n",
       "      <th>column_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SPECIMEN1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPECIMEN2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SPECIMEN3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SPECIMEN4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.66</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SPECIMEN5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.66</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    column_0  column_1  column_2  column_3  column_4  column_5  column_6\n",
       "0  SPECIMEN1       1.0     300.0      36.0      1.25        10    0.0027\n",
       "1  SPECIMEN2       2.0     450.0      48.0      1.25        10    0.0029\n",
       "2  SPECIMEN3       5.0     600.0      64.0      1.25        10    0.0022\n",
       "3  SPECIMEN4       1.0     300.0      48.0      1.66        20    0.0008\n",
       "4  SPECIMEN5       2.0     450.0      64.0      1.66        20    0.0012"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function should be triggered by a listener\n",
    "\n",
    "data_name = \"table.csv\"\n",
    "directory = '/home/aditya/Documents/citibot/newdata'\n",
    "DATASET = new_data(data_name, directory)\n",
    "\n",
    "# dict of columns in new data with corresponding dtypes\n",
    "\n",
    "DATASET['Specimen No'] = DATASET['Specimen No'].astype('str')\n",
    "DATASET.columns =[\"column \" + str(i) for i, column in enumerate(DATASET.columns)] \n",
    "DATASET.columns =[column.replace(\" \", \"_\") for column in DATASET.columns] \n",
    "\n",
    "FEATURES = {col: DATASET[col].dtype for col in DATASET}\n",
    "DATASET.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the possible entities \n",
    "def get_entities(threshold_value):\n",
    "    PRIMARY_KEY = []\n",
    "    ENTITIES = {}\n",
    "\n",
    "    for col in FEATURES.keys():\n",
    "        if (DATASET[col].unique().shape[0] == DATASET.shape[0]) and (FEATURES[col] == 'O' or FEATURES[col] == 'int64'):           \n",
    "            PRIMARY_KEY.append(col)                                             # Make this the PRIMARY KEY\n",
    "            ENTITIES[col] = DATASET[col].unique()[:threshold_value].tolist()\n",
    "\n",
    "        elif DATASET[col].unique().shape[0] > threshold_value and (FEATURES[col] == 'O' or FEATURES[col] == 'int64'):\n",
    "            ENTITIES[col] = DATASET[col].unique()[:threshold_value].tolist()\n",
    "\n",
    "        elif FEATURES[col] == 'O' or FEATURES[col] == 'int64':\n",
    "            ENTITIES[col] = DATASET[col].unique().tolist()\n",
    "            \n",
    "    return ENTITIES, PRIMARY_KEY\n",
    "\n",
    "threshold_value = 20\n",
    "ENTITIES, PRIMARY_KEY = get_entities(threshold_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'column_0': ['what is column_0 for ',\n",
       "  'Tell me something about column_0 with ',\n",
       "  'Give me information about column_0 for the '],\n",
       " 'column_1': ['what is column_1 for ',\n",
       "  'Tell me something about column_1 with ',\n",
       "  'Give me information about column_1 for the '],\n",
       " 'column_2': ['what is column_2 for ',\n",
       "  'Tell me something about column_2 with ',\n",
       "  'Give me information about column_2 for the '],\n",
       " 'column_3': ['what is column_3 for ',\n",
       "  'Tell me something about column_3 with ',\n",
       "  'Give me information about column_3 for the '],\n",
       " 'column_4': ['what is column_4 for ',\n",
       "  'Tell me something about column_4 with ',\n",
       "  'Give me information about column_4 for the '],\n",
       " 'column_5': ['what is column_5 for ',\n",
       "  'Tell me something about column_5 with ',\n",
       "  'Give me information about column_5 for the '],\n",
       " 'column_6': ['what is column_6 for ',\n",
       "  'Tell me something about column_6 with ',\n",
       "  'Give me information about column_6 for the ']}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For query generation, any number of previously determined entities (values) can be used.\n",
    "# For each intent, what should differ between the different sets of queries is the meaning the latter conveys.\n",
    "# For eg if we have an intent \"Cost\", it should have questions like :\n",
    "# what is the cost {intent} for entity_1{value} ?\n",
    "# Since we have to use the intent / Feature name as a part of the query itself, an important thing to conside is \n",
    "# that the feature names should be simple and directly convey their purpose\n",
    "# For eg, feature name can be \"cost\" but if the feature name is \"cost per head\" then we the issue is that\n",
    "# the intent name wont be the same as the feature name, once this happens we'll have to extract the real intent \n",
    "# from the feature names which is not possible to do right now.\n",
    "\n",
    "def get_questions(intent):\n",
    "    questions = []\n",
    "    questions.append(\"what is {} for [{}]({})\".format(intent, value, entity))\n",
    "    questions.append(\"Tell me something about {} with \".format(intent))\n",
    "    questions.append(\"Give me information about {} for the \".format(intent))\n",
    "    \n",
    "    return questions\n",
    "\n",
    "# Dict of intents having a list of questions as their values.\n",
    "\n",
    "INTENTS = {col:get_questions(col) for col in FEATURES.keys()}\n",
    "INTENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'table': ['column_0',\n",
       "  'column_1',\n",
       "  'column_2',\n",
       "  'column_3',\n",
       "  'column_4',\n",
       "  'column_5',\n",
       "  'column_6']}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making a dictionary of tables with corresponding intents for query based retrieval\n",
    "\n",
    "table = os.path.splitext(data_name)[0]\n",
    "\n",
    "if 'dict.pkl' not in os.listdir('/home/aditya/Documents/citibot/data'):\n",
    "    information_table = {}\n",
    "else:\n",
    "    information_table = pickle.load(open('/home/aditya/Documents/citibot/data/dict.pkl', 'rb'))\n",
    "\n",
    "information_table[table] = list(INTENTS.keys())\n",
    "pickle.dump(information_table, open('/home/aditya/Documents/citibot/data/dict.pkl', 'wb'))\n",
    "\n",
    "information_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the current nlu.md file has the following information : (1)intents, (2)lookup, (3)synonyms\n",
    "# created three functions to update each of the these sections respectively\n",
    "# optimisation for future instead of writing, just append new data\n",
    "\n",
    "\n",
    "def synonyms_to_md(data_md, entity_dict):\n",
    "    data = []\n",
    "    for entity in entity_dict.keys():\n",
    "        if FEATURES[entity] == 'int64' or entity in PRIMARY_KEY:\n",
    "            continue\n",
    "            \n",
    "        # Now the only entities left are character dtype with limited uniques\n",
    "        # For each value in entity add synonyms\n",
    "        \n",
    "        for val in entity_dict[entity]:\n",
    "            string = \" synonym:{}\\n\".format(val)\n",
    "            synonyms = [val.lower(), val.upper(), val.title()]\n",
    "            for sys in synonyms:\n",
    "                string += \"- {}\\n\".format(sys)\n",
    "            string += \"\\n\"\n",
    "            data.append(string)\n",
    "    \n",
    "    data_md += data\n",
    "    return data_md\n",
    "\n",
    "# lookup should not include primary_key entity, integer entities\n",
    "\n",
    "def lookups_to_md(data_md, entity_dict):\n",
    "    data = []\n",
    "    for entity in entity_dict.keys():\n",
    "        string_ent = \" lookup:{}\\n\".format(entity)\n",
    "        \n",
    "        if FEATURES[entity] == 'int64' or entity in PRIMARY_KEY:\n",
    "            continue\n",
    "            \n",
    "        for val in entity_dict[entity]:\n",
    "            string_ent += \"- {}\\n\".format(val)\n",
    "        string_ent += \"\\n\"\n",
    "        data.append(string_ent)\n",
    "    \n",
    "    data_md += data\n",
    "    return data_md\n",
    "\n",
    "def intents_to_md(data_md, intent_dict):\n",
    "    data_md[-1] = data_md[-1] + '\\n'\n",
    "    data = []\n",
    "    for intent in intent_dict.keys():\n",
    "        string_intent = \" intent:{}\\n\".format(intent)\n",
    "        for ques in intent_dict[intent]:\n",
    "            string_intent += \"- {}\\n\".format(ques)\n",
    "        string_intent += \"\\n\"\n",
    "        data.append(string_intent)\n",
    "    \n",
    "    data_md += data\n",
    "#     print(data)\n",
    "    return data_md\n",
    "\n",
    "nlu = open('/home/aditya/Documents/citibot/newdata/nlu.md', 'r')\n",
    "s=nlu.read().split('##')\n",
    "\n",
    "nlu_intent = intents_to_md(s, INTENTS)\n",
    "nlu_lookup = lookups_to_md(nlu_intent, ENTITIES)\n",
    "# nlu_synonyms = synonyms_to_md(nlu_lookup, ENTITIES)\n",
    "\n",
    "new_nlu = '##'.join(nlu_lookup)\n",
    "\n",
    "f = open(\"demofile2.md\", \"w\")\n",
    "f.write(new_nlu)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the current stories.md file has storylines with respect to a classified intent\n",
    "# For the dynamic addition of data, contextual storylines cannot be added due to their increased complexity\n",
    "# name : ## {intent} path\n",
    "\n",
    "def new_stories(story, intent_dict):\n",
    "    data = []\n",
    "    actions = []\n",
    "    for intent in intent_dict:\n",
    "        action = \"action_{}\".format(intent)\n",
    "        string =\" {} path 1\\n* {}\\n  - {}\\n\\n\".format(intent, intent, action)\n",
    "        data.append(string)\n",
    "        actions.append(action)\n",
    "        \n",
    "    story += data\n",
    "    return story, actions\n",
    "    \n",
    "story = open('/home/aditya/Documents/citibot/newdata/stories.md', 'r')\n",
    "s=story.read().split('##')\n",
    "\n",
    "story_text, Actions = new_stories(s, INTENTS)\n",
    "new_story = '##'.join(story_text)\n",
    "\n",
    "f = open(\"story.md\", \"w\")\n",
    "f.write(new_story)\n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['utter_column_0', 'utter_column_1']\n"
     ]
    }
   ],
   "source": [
    "# updating the actions file by addition of new actions based on an intent\n",
    "# Each of the new intent is mapped to an action which will be triggered once the latter intent is predicted\n",
    "# First open the given actions.py file in append mode, add the template text and voila new action is updated !\n",
    "text = \"\"\"\n",
    "\n",
    "class {}(Action):\n",
    "\n",
    "    def name(self) -> Text:\n",
    "        return \"{}\"\n",
    "\n",
    "    def run(self, dispatcher: CollectingDispatcher,\n",
    "            tracker: Tracker,\n",
    "            domain: Dict[Text, Any]) -> List[Dict[Text, Any]]:\n",
    "\n",
    "        try:\n",
    "            entities = tracker.latest_message['entities']\n",
    "            table = get_table(tracker.latest_message['intent'])\n",
    "            if(len(entities) == 0):\n",
    "                dispatcher.utter_message(template = '{}')\n",
    "                return []\n",
    "\n",
    "            records = record_finder(entities)\n",
    "\n",
    "            if(records.empty):\n",
    "                raise ValueError(\"No record for this query !!!\")\n",
    "    \n",
    "            print(records)\n",
    "            dispatcher.utter_message(text=\"{}\")\n",
    "            return []\n",
    "\n",
    "        except:\n",
    "            dispatcher.utter_message(text = str(sys.exc_info()[1]))\n",
    "            return []\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def add_action(Action, file):\n",
    "    utter = []\n",
    "    for action in Action[:2]:\n",
    "        utterance = \"utter\" + action[6:]\n",
    "        class_name = action.replace(\"_\", \"\")\n",
    "        utter.append(utterance)\n",
    "        template = text.format(class_name, action, utterance, \"its working for \")\n",
    "        file.write(template)\n",
    "#         print(template)\n",
    "    \n",
    "    return utter\n",
    "\n",
    "current_actions = open('/home/aditya/Documents/ipynb/actions.py', 'a', encoding = 'utf-8')\n",
    "utterances = add_action(Actions, current_actions)\n",
    "print(utterances)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating the domain.yml file\n",
    "# it contains a list of intents, actions, responses, entities, slots\n",
    "\n",
    "def update_domain(intents, actions, entities, responses, data):\n",
    "    \n",
    "    # update intent list\n",
    "    pointer_intent = data.find(\"intents:\")\n",
    "    string = \"\"\n",
    "    for intent in intents:\n",
    "        string += \"  - {}\\n\".format(intent)\n",
    "    data = data[:pointer_intent+9] + string + data[pointer_intent+9:]\n",
    "    \n",
    "    # update actions list \n",
    "    pointer_actions = data.find(\"actions:\")\n",
    "    string = \"\"\n",
    "    for action in actions[:2]:\n",
    "        string += \"  - {}\\n\".format(action)\n",
    "    data = data[:pointer_actions+9] + string + data[pointer_actions+9:]\n",
    "    \n",
    "    # update entity list\n",
    "    pointer_ent = data.find(\"entities:\")\n",
    "    string = \"\"\n",
    "    for entity in entities:\n",
    "        string += \"  - {}\\n\".format(entity)\n",
    "    data = data[:pointer_ent+10] + string + data[pointer_ent+10:]\n",
    "    \n",
    "    # update slot list\n",
    "#     pointer_ent = data.find(\"slots:\")\n",
    "#     string = \"\"\n",
    "#     for entity in entities:\n",
    "#         string += \"  - {}\\n\".format(entity)\n",
    "#     data = data[:pointer_ent+10] + string + data[pointer_ent+10:]\n",
    "    # update response list\n",
    "#     pointer_ent = data.find(\"responses:\")\n",
    "#     string = \"\"\n",
    "#     for entity in entities:\n",
    "#         string += \"  - {}\\n\".format(entity)\n",
    "#     data = data[:pointer_ent+10] + string + data[pointer_ent+10:]\n",
    "    \n",
    "    return data\n",
    "\n",
    "domain = open('/home/aditya/Documents/ipynb/domain.yml', 'r').read()\n",
    "new_domain = update_domain(list(INTENTS.keys()), Actions, list(ENTITIES.keys()), utterances, domain)\n",
    "# print(domain[domain.find(\"actions:\"): domain.find(\"actions:\")+9] + \"jj\\n\" +domain[domain.find(\"actions:\") + 9:])\n",
    "# print(domain)\n",
    "f = open(\"domain.yml\", \"w\")\n",
    "f.write(new_domain)\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "class ActionPayment(Action):\n",
    "\n",
    "    def name(self) -> Text:\n",
    "        return {}\n",
    "\n",
    "    def run(self, dispatcher: CollectingDispatcher,\n",
    "            tracker: Tracker,\n",
    "            domain: Dict[Text, Any]) -> List[Dict[Text, Any]]:\n",
    "\n",
    "        try:\n",
    "            entities = tracker.latest_message['entities']\n",
    "            table = get_table(tracker.latest_message['intent'])\n",
    "            if(len(entities) == 0):\n",
    "                dispatcher.utter_message(template = {})\n",
    "                return []\n",
    "\n",
    "            records = record_finder(entities)\n",
    "\n",
    "            if(records.empty):\n",
    "                raise ValueError(\"No record for this query !!!\")\n",
    "    \n",
    "            print(records)\n",
    "            dispatcher.utter_message(text={} +records[{}].item())# {}  {}\".format(PS, LE))\n",
    "            return [{}]\n",
    "\n",
    "        except:\n",
    "            dispatcher.utter_message(text = str(sys.exc_info()[1]))\n",
    "            return []\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "file = open('/home/aditya/Documents/ipynb/actions.py', 'a', encoding = 'utf-8')\n",
    "file.write(text)\n",
    "file.close()\n",
    "\n",
    "mdf = open('/home/aditya/Documents/citibot/newdata/nlu.md', 'r')\n",
    "# markdown.markdown(mdf.read())\n",
    "s=mdf.read().split('##')\n",
    "\n",
    "h, p = \"heyaaaa\", \"how\"\n",
    "string = \"\"\n",
    "\n",
    "string1 = \" intent:{}\\n\".format(h)\n",
    "string1 += \"- {}\\n\".format(p)\n",
    "string1 += \"\\n\"\n",
    "\n",
    "string2 = \" intent:{}\\n\".format(p)\n",
    "string2 += \"- {}\\n\".format(h)\n",
    "string2 += \"\\n\"\n",
    "ss= []\n",
    "ss.append(string1)\n",
    "ss.append(string2)\n",
    "print(ss)\n",
    "v = '##'.join([string, string1, string2])\n",
    "\n",
    "s[-1] = s[-1] + '\\n'\n",
    "s += ss\n",
    "print(s)\n",
    "v = '##'.join(s)\n",
    "\n",
    "text = \"adbc {}\"\n",
    "x=1\n",
    "t = text.format(\"8\")\n",
    "print(t)\n",
    "text = \"action_intent\"\n",
    "\"utter\"+text[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
